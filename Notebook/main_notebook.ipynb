{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":90101,"databundleVersionId":10457622,"sourceType":"competition"}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Innovative AI Challenge 2024\n\n<img src=\"https://github.com/Sakib-Dalal/Crop_Recommendation_AI_App/blob/main/app/client/public/images/Logo.jpg?raw=true\" />\n\n#### Project By Sakib Dalal\n- GitHub Project repo: <a href=\"www.github.com\">Link</a>\n#### problem statement: \n- AI in Agriculture: Develop AI models to enhance intensive agricultural practices and address the future global food crisis.\n\n#### Agriculture Productivity Prediction\n- **Objective**: Build an AI/ML model that predicts agricultural productivity based on crop type, weather conditions, soil properties, and other relevant factors.\n- **Requirements**:\n    - Ensure the model is accurate and farmer-friendly.\n    - Provide a simple, accessible user interface for farmers to use effectively. Create an interface or a small website to showcase your AI/ML model.\n    - Problem analysis, solution overview, methodology, and implementation steps. Short video, codes via challenge website.\n- **Scoring**:\n    - Submissions will be evaluated based on *Mean Squared Error*.","metadata":{}},{"cell_type":"markdown","source":"### Challenge Overview\n\n#### Files\n- **train.csv** - The training dataset, which includes the features and target variable (crop_yield in kg/ha).\n- **test.csv** - The test dataset, which you will use to generate predictions and submit your solutions.\n- **sample_submission.csv** - A sample submission file that shows the correct format for submitting your predictions.\n#### Columns\n- `id' - A unique identifier for each data point (e.g., 1, 2, 3,â€¦).\n- 'Year' - The year of the production (e.g., 2020, 2002).\n- 'State' - The state where the data is collected (e.g., Punjab).\n- 'Crop_Type' - The type of crop grown (e.g., Rice, Wheat, Bajra).\n- 'Rainfall' - The amount of annual average state rainfall in mm (e.g., 1200 mm).\n- 'Soil_Type' - The type of soil in the region (e.g., Loamy).\n- 'Irrigation_Area' - Area of irrigated land in Thousand hectare\n- 'Crop_Yield' - The target variable representing the crop yield in kg/ha.\n#### Notes:\n- Data Format: The data is provided in CSV format. Ensure that all files are read correctly and that you handle any missing data appropriately.\n- Feature Engineering: While the data is provided in a raw form, you may perform feature engineering and transformations to enhance your model.\n- Prediction Goal: Your model should predict agricultural productivity based on the features in the data (for the agriculture problem statement).\n\n<img src=\"https://images.javatpoint.com/tutorial/machine-learning/images/machine-learning-life-cycle.png\" />","metadata":{}},{"cell_type":"markdown","source":"# Index\n\n- **Step 1**: Loading Dataset\n    - The dataset is provided by the Innovative AI Challenge 2024\n    - Link for data set on kaggle: <a href=\"https://www.kaggle.com/competitions/innovative-ai-challenge-2024/data\">dataset</a>\n- **Step 2**: Data Preparation\n    - We will prepare our data in this step.\n    - The dataset is provided into *csv* format.\n    - using **polars** we will convert into dataframe.\n- **Step 3**: Data Wrangling\n    - Look for any **null** values in dataset.\n- **Step 4**: Analyse Data\n    - In this step we will *Analyse* and *Visualise* our data.\n    - Here we will select important features to train our *ML* model.\n- **Step 5**: Training Model\n    - We will use diffent *ML* model's for our *regression problem*.\n    - Based on the evaluation we will select a model for testing.\n- **Step 6**: Testing Model\n    - It it the phase where we will finalize and predict the test data.\n- **Step 7**: Deployment\n    - Saving best performing model so that we can use in our **Web App**.","metadata":{}},{"cell_type":"markdown","source":"# Step 1: Importing Data\n- We will import the dataset from kaggle\n- The dataset is in csv format.\n- here are the links for dataset:\n    - train.csv: \"/kaggle/input/innovative-ai-challenge-2024/train.csv\"\n    - test.csv: \"/kaggle/input/innovative-ai-challenge-2024/test.csv\"\n    - sample_submission.csv: \"/kaggle/input/innovative-ai-challenge-2024/sample_submission.csv\"","metadata":{}},{"cell_type":"code","source":"TRAIN_URL = \"/kaggle/input/innovative-ai-challenge-2024/train.csv\"\nTEST_URL = \"/kaggle/input/innovative-ai-challenge-2024/test.csv\"\nSUBMISSION_URL = \"/kaggle/input/innovative-ai-challenge-2024/sample_submission.csv\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T07:42:26.980377Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Step 2: Data Preparation\n- We will be using <a href=\"https://pandas.pydata.org/pandas-docs/stable/index.html\">Pandas</a> to convert our dataset from *csv* to *dataframes*.\n- **Note**: For future proof project better use **Polars** for data processing.\n- We will use **read_csv** method from Pandas to read the csv file and convert into dataframe.","metadata":{}},{"cell_type":"code","source":"import pandas as pd","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df = pd.read_csv(TRAIN_URL)\ntest_df = pd.read_csv(TEST_URL)\n\nsubmission_df = pd.read_csv(SUBMISSION_URL)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- Let's view the first five items in our dataset's","metadata":{}},{"cell_type":"code","source":"train_df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission_df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- Let get from training data\n    - information\n    - number of columns\n    - shape and size\n    - description","metadata":{}},{"cell_type":"code","source":"train_df.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.columns","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(train_df.columns)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- From this we get to now:\n    - There are total 8 columns\n    - **Crop_Yield (kg/ha)** is the target columns\n    - There are 55 rows.","metadata":{}},{"cell_type":"code","source":"train_df.describe()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- let's see unique values and value counts in Year column from `train_df`","metadata":{}},{"cell_type":"code","source":"train_df[\"Year\"].unique()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df[\"Year\"].value_counts()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- Let's see unique values and value count's in State column in train_df","metadata":{}},{"cell_type":"code","source":"train_df[\"State\"].unique()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df[\"State\"].value_counts()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- Let's see unique values and value count's in Crop Type column in train_df","metadata":{}},{"cell_type":"code","source":"train_df[\"Crop_Type\"].unique()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df[\"Crop_Type\"].value_counts()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- Let's see unique values and value count's in Soil_Type column in train_df","metadata":{}},{"cell_type":"code","source":"train_df[\"Soil_Type\"].unique()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df[\"Soil_Type\"].value_counts()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Step 3: Data Wrangling\n- In this step we will look for any null values in our *training* dataset.\n- For this we will be using **Pandas** library","metadata":{}},{"cell_type":"code","source":"train_df.isna().sum()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- Hence, there are no *null* values in our training dataset.\n- Let's see the datatype of each columns in train_df.","metadata":{}},{"cell_type":"code","source":"train_df.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for k, v in train_df.items():\n    print(k,\"column has datatype of:\", v.dtype)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# print columns with datatype as object\nfor k, v in train_df.items():\n    if v.dtype == \"object\":\n        print(k,\"column has datatype of:\", v.dtype)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- There are 3 types of data (float64, int64, object)\n- We can train our model on float64, int64 datatype but we get an error when we pass object datatype to our model\n- **Solution**: To overcome this problem we can use **Sklearn Preprocessing's** *<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html\"> LabelEncoder </a>*.","metadata":{}},{"cell_type":"code","source":"# Label Encoder to deal with object datatypes\nfrom sklearn.preprocessing import LabelEncoder","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- before moving one we create a copy of `train_df` dataframe so we can reuse the original when we needed.","metadata":{}},{"cell_type":"code","source":"train_cp_df = train_df.copy()\ntrain_cp_df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"label_encoder = LabelEncoder()\n\n# Encoding labels in columns\ntrain_cp_df[\"State\"] = label_encoder.fit_transform(train_cp_df[\"State\"])\ntrain_cp_df[\"Crop_Type\"] = label_encoder.fit_transform(train_cp_df[\"Crop_Type\"])\ntrain_cp_df[\"Soil_Type\"] = label_encoder.fit_transform(train_cp_df[\"Soil_Type\"])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- `train_cp_df` dataset after *LabelEncoding*.","metadata":{}},{"cell_type":"code","source":"train_cp_df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for k, v in train_cp_df.items():\n    print(k,\"column has datatype of:\", v.dtype)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# old dataset info\nfor k, v in train_df.items():\n    print(k,\"column has datatype of:\", v.dtype)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- We have successfuly encoded the object datatypes in out dataset.","metadata":{}},{"cell_type":"markdown","source":"### Feature Scaling\n- our `train_cp_df` dataset contains features that highly vary in magnitudes, units, and range.\n- here is visualization for all the features.\n- We will be using matplotlib and seaborn for visualization.","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set_style(\"darkgrid\")\n\ntrain_cp_df.plot(figsize=(16, 6))\nplt.title(\"Before Feature Scaling\", fontsize=16)\nplt.xlabel(\"X Scale\")\nplt.ylabel(\"Y Scale\")\n\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- Here is the histogram view.","metadata":{}},{"cell_type":"code","source":"train_cp_df.hist(figsize=(16, 8), bins=20, color=[\"salmon\"])\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- Let's perform feature scaling on out `train_cp_df` dataset.\n- We will be using Sklearn's preprocessing method named **<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\">StandardScaler</a>**\n- Formula to perform standard scaling is:<br> $z = (x - u) / s$\n- where:\n    - z is scaled data\n    - x is to be scaled data\n    - u is the mean of the training samples\n    - s is the standard deviation of the training samples.\n- There are also different scaling method provided by sklearn but StandardScaler perform the best.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"scaler = StandardScaler()\n\n# we will apply feature scaling on train_cp_df dataset\ntrain_cp_df = scaler.fit_transform(train_cp_df)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- after standard scaling our data will be no more in dataframe, it will be converted into numpy arrays.\n- we will again use the pandas to convert it into dataframe.","metadata":{}},{"cell_type":"code","source":"type(train_cp_df)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_cp_df = pd.DataFrame(data=train_cp_df, columns=train_df.columns)\ntrain_cp_df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# plot after feature scaling\ntrain_cp_df.plot(figsize=(16, 6))\nplt.title(\"After Feature Scaling\", fontsize=16)\nplt.xlabel(\"X Scale\")\nplt.ylabel(\"Y Scale\")\n\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- Now it's better. Each feature had scaled between 2 to -2.\n- here is the histogram view.","metadata":{}},{"cell_type":"code","source":"train_cp_df.hist(figsize=(16, 8), bins=20, color=[\"salmon\"])\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- Now we can easy extract the important features from dataset.","metadata":{}},{"cell_type":"markdown","source":"### Extracting Important Features \n- In this step we will select the best features so that our model will perform better.\n- In first method of feature extraction we will be using C","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import polars as pl","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/innovative-ai-challenge-2024/train.csv\")\ndata","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load data using panadas\ntrain_df = pd.read_csv('/kaggle/input/innovative-ai-challenge-2024/train.csv')\ntest_df = pd.read_csv('/kaggle/input/innovative-ai-challenge-2024/test.csv')\nsubmission_df= pd.read_csv('/kaggle/input/innovative-ai-challenge-2024/sample_submission.csv')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n# Correlation Matrix\nplt.figure(figsize=(10, 6))\n# Select only numeric columns for correlation calculation\nnumeric_data = train_df.select_dtypes(include=np.number)\ncorrelation_matrix = numeric_data.corr()  \nsns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\")\nplt.title(\"Correlation Matrix\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Import label encoder \nfrom sklearn import preprocessing \n  \n# label_encoder object knows  \n# how to understand word labels. \nlabel_encoder = preprocessing.LabelEncoder() \n  \n# Encode labels in column 'species'. \ntrain_df['Soil_Type']= label_encoder.fit_transform(train_df['Soil_Type']) \ntrain_df['Crop_Type']= label_encoder.fit_transform(train_df['Crop_Type']) \ntrain_df['State']= label_encoder.fit_transform(train_df['State']) ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\n# Assuming df is your DataFrame and 'target_variable' is the column you want to predict\nX = train_df.drop([\"Crop_Yield (kg/ha)\", \"State\", \"id\", \"Year\"], axis=1)\ny = train_df[\"Crop_Yield (kg/ha)\"]\n\n# Applying RandomForestRegressor\nmodel = RandomForestRegressor(n_estimators=100, random_state=42)\nmodel.fit(X, y)\n\n# Displaying feature importance\nfeature_importance = pd.DataFrame({'Feature': X.columns, 'Importance': model.feature_importances_})\nprint(feature_importance.sort_values(by='Importance', ascending=False))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission_df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}